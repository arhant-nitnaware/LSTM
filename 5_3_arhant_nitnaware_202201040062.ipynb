{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "# Paste this entire block into one notebook cell (or script) and run top to bottom\n",
        "# ────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "# 1. Install & import libraries\n",
        "!pip install -q tensorflow tensorflow-datasets numpy matplotlib\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# 2. Load & concatenate the Shakespeare corpus\n",
        "print(\"Loading tiny_shakespeare dataset...\")\n",
        "ds = tfds.load('tiny_shakespeare', split='train', shuffle_files=True)\n",
        "text = ''.join([ex['text'].decode('utf-8') for ex in tfds.as_numpy(ds)])\n",
        "print(f\"Total characters: {len(text)}\")\n",
        "\n",
        "# 3. Build vocabulary & encode text\n",
        "vocab      = sorted(set(text))\n",
        "vocab_size = len(vocab)\n",
        "print(f\"Vocab size: {vocab_size}\")\n",
        "char2idx = {ch: i for i, ch in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "text_as_int = np.array([char2idx[c] for c in text], dtype=np.int32)\n",
        "\n",
        "# 4. Hyperparameters & train/test split\n",
        "seq_length  = 100\n",
        "BATCH_SIZE  = 64\n",
        "BUFFER_SIZE = 10000\n",
        "EPOCHS      = 5\n",
        "TEST_SPLIT  = 0.9\n",
        "\n",
        "split_idx   = int(len(text_as_int) * TEST_SPLIT)\n",
        "train_ints  = text_as_int[:split_idx]\n",
        "test_ints   = text_as_int[split_idx:]\n",
        "\n",
        "# 5. Dataset builder\n",
        "def make_dataset(arr, seq_length, batch_size, buffer_size=0, shuffle=False):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(arr)\n",
        "    ds = ds.batch(seq_length + 1, drop_remainder=True)\n",
        "    ds = ds.map(lambda chunk: (chunk[:-1], chunk[1:]))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size)\n",
        "    return ds.batch(batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_ds = make_dataset(train_ints, seq_length, BATCH_SIZE, BUFFER_SIZE, shuffle=True)\n",
        "test_ds  = make_dataset(test_ints,  seq_length, BATCH_SIZE)\n",
        "\n",
        "# 6. Build & compile the model (with a defined input so params are created)\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\n",
        "    tf.keras.layers.Embedding(vocab_size, 256),\n",
        "    tf.keras.layers.LSTM(1024, return_sequences=True),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model.summary()   # now shows non-zero params\n",
        "\n",
        "# 7. Checkpoint callback (filename must end in .weights.h5 when save_weights_only=True)\n",
        "checkpoint_dir  = './training_checkpoints'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "ckpt_filepath  = os.path.join(checkpoint_dir, 'ckpt_{epoch}.weights.h5')\n",
        "cp_callback     = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=ckpt_filepath,\n",
        "    save_weights_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 8. Train\n",
        "history = model.fit(train_ds, epochs=EPOCHS, callbacks=[cp_callback])\n",
        "\n",
        "# 9. Evaluate on test split\n",
        "test_loss, test_acc = model.evaluate(test_ds)\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "5i_KSHTGQlap",
        "outputId": "e405aac9-ad3a-4dda-ee35-80cf8a5683c5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tiny_shakespeare dataset...\n",
            "Total characters: 1003854\n",
            "Vocab size: 65\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_5 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │          \u001b[38;5;34m16,640\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)          │       \u001b[38;5;34m5,246,976\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65\u001b[0m)            │          \u001b[38;5;34m66,625\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,640</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)          │       <span style=\"color: #00af00; text-decoration-color: #00af00\">5,246,976</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">66,625</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,330,241\u001b[0m (20.33 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,330,241</span> (20.33 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,330,241\u001b[0m (20.33 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,330,241</span> (20.33 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.1924 - loss: 3.2337\n",
            "Epoch 1: saving model to ./training_checkpoints/ckpt_1.weights.h5\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 69ms/step - accuracy: 0.1929 - loss: 3.2302\n",
            "Epoch 2/5\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.3873 - loss: 2.1121\n",
            "Epoch 2: saving model to ./training_checkpoints/ckpt_2.weights.h5\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 71ms/step - accuracy: 0.3874 - loss: 2.1115\n",
            "Epoch 3/5\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.4665 - loss: 1.8074\n",
            "Epoch 3: saving model to ./training_checkpoints/ckpt_3.weights.h5\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 73ms/step - accuracy: 0.4666 - loss: 1.8070\n",
            "Epoch 4/5\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.5138 - loss: 1.6306\n",
            "Epoch 4: saving model to ./training_checkpoints/ckpt_4.weights.h5\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 73ms/step - accuracy: 0.5138 - loss: 1.6304\n",
            "Epoch 5/5\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.5424 - loss: 1.5228\n",
            "Epoch 5: saving model to ./training_checkpoints/ckpt_5.weights.h5\n",
            "\u001b[1m139/139\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 75ms/step - accuracy: 0.5424 - loss: 1.5227\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.5339 - loss: 1.5488\n",
            "Test Loss: 1.5816, Test Accuracy: 0.5254\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ─── Cell 10: Reload weights & generate ─────────────────────────────────────\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# 1. Find the latest .weights.h5 file\n",
        "weight_files   = glob.glob(os.path.join(checkpoint_dir, '*.weights.h5'))\n",
        "latest_weights = max(weight_files, key=os.path.getctime)\n",
        "print(f\"Loading weights from: {latest_weights}\")\n",
        "\n",
        "# 2. Rebuild the same model architecture\n",
        "gen_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(None,), dtype=tf.int32),\n",
        "    tf.keras.layers.Embedding(vocab_size, 256),\n",
        "    tf.keras.layers.LSTM(1024, return_sequences=True),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "])\n",
        "\n",
        "# 3. Load weights\n",
        "gen_model.load_weights(latest_weights)\n",
        "\n",
        "# 4. Text generation function\n",
        "def generate_text(model, start_string, num_generate=300, temperature=0.5):\n",
        "    input_eval = tf.expand_dims([char2idx[s] for s in start_string], 0)\n",
        "    generated = []\n",
        "    for _ in range(num_generate):\n",
        "        preds = model(input_eval)\n",
        "        preds = preds[:, -1, :] / temperature\n",
        "        pred_id = tf.random.categorical(preds, num_samples=1)[0,0].numpy()\n",
        "        generated.append(idx2char[pred_id])\n",
        "        input_eval = tf.expand_dims([pred_id], 0)\n",
        "    return start_string + ''.join(generated)\n",
        "\n",
        "# 5. Generate & display\n",
        "print(generate_text(gen_model, start_string=\"ROMEO: \", num_generate=300, temperature=0.5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgcIxr9dTj_v",
        "outputId": "676a84f0-2310-4cf2-b06d-a19ed1f2310d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading weights from: ./training_checkpoints/ckpt_5.weights.h5\n",
            "ROMEO: IS:\n",
            "TENERINoures whengrandorererong;\n",
            "\n",
            "Then s waly tharorinor watho mis chand th ingit thileared s we angat the hous we ar the t t here the tharore the thor the there s t te hen fathoulaind hathe he whe t byorengn me wholed are there thes he the the the t mere he th har y\n",
            "\n",
            "Thithealan hea the r the y \n"
          ]
        }
      ]
    }
  ]
}